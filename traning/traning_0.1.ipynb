{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50cca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebb5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"B:\\\\CV Drone Detection\\\\scaled\\\\train\"\n",
    "PATH_TEST = \"B:\\\\CV Drone Detection\\\\scaled\\\\test\"\n",
    "PATH_VALID = \"B:\\\\CV Drone Detection\\\\scaled\\\\valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cd91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['drone', 'notAdrone']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),   # різні масштаби об’єкта\n",
    "    transforms.RandomHorizontalFlip(),                     # віддзеркалення\n",
    "    transforms.RandomVerticalFlip(),                       \n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomRotation(20),                         \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.Resize((128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.2)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=PATH_TRAIN, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=PATH_TEST, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(\"Classes:\", classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821c5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({0: 8740, 1: 5563})\n",
      "Valid: Counter({1: 650, 0: 339})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Train:\", Counter(trainset.targets))\n",
    "print(\"Valid:\", Counter(testset.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9f654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(5, 5)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, len(classes))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "#         x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = nn.functional.relu(self.fc1(x))\n",
    "#         x = nn.functional.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efe7dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 128, 128)\n",
    "            out = self.pool(F.relu(self.conv1(dummy)))\n",
    "            out = self.pool(F.relu(self.conv2(out)))\n",
    "            n_features = out.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_features, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 30 * 30)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net(num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b9f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "354bfcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.354\n",
      "[2,  2000] loss: 0.199\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "918a2b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the  test images: 93 % 929 989\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the  test images: %d %%' % (\n",
    "    100 * correct / total), correct, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "998acce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       drone       0.88      0.91      0.90       339\n",
      "   notAdrone       0.95      0.94      0.95       650\n",
      "\n",
      "    accuracy                           0.93       989\n",
      "   macro avg       0.92      0.93      0.92       989\n",
      "weighted avg       0.93      0.93      0.93       989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6bd3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = {\n",
    "    \"notAdrone\": glob.glob(r'B:\\CV Drone Detection\\scaled\\valid\\notAdrone\\*.jpg'),\n",
    "    \"drone\": glob.glob(r'B:\\CV Drone Detection\\scaled\\valid\\drone\\*.jpg')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d000e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       drone       1.00      0.82      0.90       721\n",
      "   notAdrone       0.72      1.00      0.83       325\n",
      "\n",
      "    accuracy                           0.88      1046\n",
      "   macro avg       0.86      0.91      0.87      1046\n",
      "weighted avg       0.91      0.88      0.88      1046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "class_to_idx = trainset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "def multi_glob(pattern_base):\n",
    "    return (\n",
    "        glob.glob(pattern_base + \"*.jpg\") +\n",
    "        glob.glob(pattern_base + \"*.jpeg\") +\n",
    "        glob.glob(pattern_base + \"*.png\")\n",
    "    )\n",
    "\n",
    "image_paths = {\n",
    "    \"drone\":     multi_glob(r\"B:\\CV Drone Detection\\scaled\\valid\\drone\\\\\"),\n",
    "    \"notAdrone\": multi_glob(r\"B:\\CV Drone Detection\\scaled\\valid\\notAdrone\\\\\"),\n",
    "}\n",
    "\n",
    "device = next(net.parameters()).device\n",
    "def model_testing(image_paths_dict, net, transform):\n",
    "    net.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for class_name, paths in image_paths_dict.items():\n",
    "            class_idx = class_to_idx[class_name]\n",
    "            for p in paths:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                inp = transform(img).unsqueeze(0).to(device)\n",
    "                out = net(inp)\n",
    "                pred = out.argmax(dim=1).item()\n",
    "                y_true.append(class_idx)\n",
    "                y_pred.append(pred)\n",
    "    return y_true, y_pred\n",
    "\n",
    "y_true, y_pred = model_testing(image_paths, net, val_transform)\n",
    "target_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
